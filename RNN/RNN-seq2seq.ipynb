{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpcvrvFY1EAIPV+ulFq3zt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbL_acHQr6Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAu6r7Sr8_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nw0adM1sDn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "   return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "   indexes = indexesFromSentence(lang, sentence)\n",
        "   indexes.append(EOS_token)\n",
        "   return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "   input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "   target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "   return (input_tensor, target_tensor)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XovrkMmSsJgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
        "       super(Encoder, self).__init__()\n",
        "      \n",
        "       #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
        "       self.input_dim = input_dim\n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       #initialize the embedding layer with input and embbed dimention\n",
        "       self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
        "       #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
        "       #set the number of gru layers\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "              \n",
        "   def forward(self, src):\n",
        "      \n",
        "       embedded = self.embedding(src).view(1,1,-1)\n",
        "       outputs, hidden = self.gru(embedded)\n",
        "       return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "#set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "# initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "# reshape the input to (1, batch_size)\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       output, hidden = self.gru(embedded, hidden)       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "      \n",
        "       return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG1ba_8XsKa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "    model_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    loss = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    output = model(input_tensor, target_tensor)\n",
        "\n",
        "    num_iter = output.size(0)\n",
        "\n",
        "#calculate the loss from a predicted sentence with the expected result\n",
        "    for ot in range(num_iter):\n",
        "        loss += criterion(output[ot], target_tensor[ot])\n",
        "\n",
        "    loss.backward()\n",
        "    model_optimizer.step()\n",
        "    epoch_loss = loss.item() / num_iter\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def trainModel(model, source, target, pairs, num_iteration=20000):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.NLLLoss()\n",
        "    total_loss_iterations = 0\n",
        "    training_pairs = [tensorsFromPair(source, target, random.choice(pairs)) for i in range(num_iteration)]\n",
        "    \n",
        "    print(\"num of iterations is\", num_iteration)\n",
        "    for iter in range(1, num_iteration+1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        total_loss_iterations += clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "\n",
        "        if iter % 50 == 0:\n",
        "            avarage_loss= total_loss_iterations / 50\n",
        "            total_loss_iterations = 0\n",
        "            print('%d %.4f' % (iter, avarage_loss))\n",
        "        \n",
        "            \n",
        "    torch.save(model.state_dict(), 'mytraining.pt')\n",
        "    return model"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJt3wenHsOgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
        "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
        "    \n",
        "        decoded_words = []\n",
        "    \n",
        "        output = model(input_tensor, output_tensor)\n",
        "        # print(output_tensor)\n",
        "    \n",
        "        for ot in range(output.size(0)):\n",
        "            topv, topi = output[ot].topk(1)\n",
        "            # print(topi)\n",
        "\n",
        "            if topi[0].item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, source, target, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print(\"source {}\".format(pair[0]))\n",
        "        print(\"target {}\".format(pair[1]))\n",
        "        output_words = evaluate(model, source, target, pair)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print(\"predicted {}\".format(output_sentence))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShDRuPLar94_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalize every sentence\n",
        "def normalize_sentence(df, lang):\n",
        "    sentence = df[lang].str\n",
        "    sentence = sentence.encode('utf-8', errors='ignore').str.decode('utf-8')\n",
        "    return sentence\n",
        "\n",
        "def read_sentence(df, lang1, lang2):\n",
        "   sentence1 = normalize_sentence(df, lang1)\n",
        "   sentence2 = normalize_sentence(df, lang2)\n",
        "   return sentence1, sentence2\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "   df = pd.read_csv(loc, delimiter=' , ', header=None, names=[lang1, lang2])\n",
        "   return df\n",
        "\n",
        "def process_data(lang1,lang2):\n",
        "   df = read_file('/content/ferdosi.txt', lang1, lang2)\n",
        "   print(\"Read %s sentence pairs\" % len(df))\n",
        "   sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "\n",
        "   source = Lang()\n",
        "   target = Lang()\n",
        "   pairs = []\n",
        "   for i in range(len(df)):\n",
        "       if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
        "           full = [sentence1[i], sentence2[i]]\n",
        "           source.addSentence(sentence1[i])\n",
        "           target.addSentence(sentence2[i])\n",
        "           pairs.append(full)\n",
        "\n",
        "   return source, target, pairs"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp3uE7clsROk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "779d47d8-8120-4b1d-9187-3e09de529c41"
      },
      "source": [
        "lang1 = 'eng'\n",
        "lang2 = 'ind'\n",
        "source, target, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "#print number of words\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 30000\n",
        "\n",
        "#create encoder-decoder model\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "#print model \n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, source, target, pairs, num_iteration)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Read 49609 sentence pairs\n",
            "random sentence ['سخن راند از راه و رنج دراز', 'که چون راندی اندر نشیب و فراز']\n",
            "49609\n",
            "Input : 12458 Output : 13071\n",
            "Encoder(\n",
            "  (embedding): Embedding(12458, 256)\n",
            "  (gru): GRU(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(13071, 256)\n",
            "  (gru): GRU(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=13071, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "num of iterations is 30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5297b5e8fa0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-2f9332b9a288>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, source, target, pairs, num_iteration)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtotal_loss_iterations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclacModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-2f9332b9a288>\u001b[0m in \u001b[0;36mclacModel\u001b[0;34m(model, input_tensor, target_tensor, model_optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYzgdzQdsadq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "def75e40-5cbd-49a5-c14b-9efc761c47c3"
      },
      "source": [
        "evaluateRandomly(model, source, target, pairs)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source به پیش همه موبدان سرو گفت\n",
            "target که زیبا بود ماه را شاه جفت\n",
            "predicted که و و و و <EOS>\n",
            "source و زان روی خاقان بقلب اندرون\n",
            "target ز پیلان زمین چون که بیستون\n",
            "predicted که و و و و <EOS>\n",
            "source پشین بود از تخمه کیقباد\n",
            "target خردمند شاهی دلش پر ز داد\n",
            "predicted که و و و و <EOS>\n",
            "source کسی کاین جهان داد دیگر دهد\n",
            "target نه بر من سپاسی همی برنهد\n",
            "predicted که و و و و <EOS>\n",
            "source همی راند تا پیش التوینه\n",
            "target سپاهی سبک بی نیاز از بنه\n",
            "predicted که و و و و <EOS>\n",
            "source چو زین باز گردی بیارای جنگ\n",
            "target منم ساخته جنگ را چون پلنگ\n",
            "predicted که و و و و <EOS>\n",
            "source که داند که فردا چه شاید بدن\n",
            "target بدین داستانی نباید زدن\n",
            "predicted که و و و و\n",
            "source وگر جنگ جوید کسی با سپاه\n",
            "target دل کینه دارش نیاید براه\n",
            "predicted که و و و و <EOS>\n",
            "source چنین گفت گویا گشسپ دبیر\n",
            "target که ای نامداران برنا و پیر\n",
            "predicted که و و و و <EOS>\n",
            "source یکی گفت کای شاه خرم نهان\n",
            "target سخن راندی چند پیش مهان\n",
            "predicted که و و و و <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4ur2Qd1AbtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}